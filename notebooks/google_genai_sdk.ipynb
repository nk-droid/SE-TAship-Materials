{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f42e93d0",
   "metadata": {},
   "source": [
    "## 🔧 Install Dependencies\n",
    "We need the `google-generativeai` SDK to access Gemini models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d4b1e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\HP\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q google-generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464e0d9a",
   "metadata": {},
   "source": [
    "## 🔑 Configure API Key\n",
    "You’ll need a Google AI Studio API key.  \n",
    "1. Go to [Google AI Studio](https://aistudio.google.com/)  \n",
    "2. Generate an API key  \n",
    "3. Store it in your environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebf088d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Set your API key (make sure you stored it as environment variable)\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2310cf",
   "metadata": {},
   "source": [
    "## 📋 List Available Models\n",
    "This helps us see what Gemini models are accessible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ffc3415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-gecko-001 \n",
      "- supports  ['embedText', 'countTextTokens']\n",
      "\n",
      "models/gemini-2.5-pro-preview-03-25 \n",
      "- supports  ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "\n",
      "models/gemini-2.5-flash-preview-05-20 \n",
      "- supports  ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "\n",
      "models/gemini-2.5-flash \n",
      "- supports  ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "\n",
      "models/gemini-2.5-flash-lite-preview-06-17 \n",
      "- supports  ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "\n",
      "models/gemini-2.5-pro-preview-05-06 \n",
      "- supports  ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "\n",
      "models/gemini-2.5-pro-preview-06-05 \n",
      "- supports  ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "\n",
      "models/gemini-2.5-pro \n",
      "- supports  ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "\n",
      "models/gemini-2.0-flash-exp \n",
      "- supports  ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "\n",
      "models/gemini-2.0-flash \n",
      "- supports  ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "\n",
      "models/gemini-2.0-flash-001 \n",
      "- supports  ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "\n",
      "models/gemini-2.0-flash-exp-image-generation \n",
      "- supports  ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "\n",
      "models/gemini-2.0-flash-lite-001 \n",
      "- supports  ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "\n",
      "models/gemini-2.0-flash-lite \n",
      "- supports  ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "\n",
      "models/gemini-2.0-flash-preview-image-generation \n",
      "- supports  ['generateContent', 'countTokens', 'batchGenerateContent']\n",
      "\n",
      "models/gemini-2.0-flash-lite-preview-02-05 \n",
      "- supports  ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "\n",
      "models/gemini-2.0-flash-lite-preview \n",
      "- supports  ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "\n",
      "models/gemini-2.0-pro-exp \n",
      "- supports  ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "\n",
      "models/gemini-2.0-pro-exp-02-05 \n",
      "- supports  ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "\n",
      "models/gemini-exp-1206 \n",
      "- supports  ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "\n",
      "models/gemini-2.0-flash-thinking-exp-01-21 \n",
      "- supports  ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "\n",
      "models/gemini-2.0-flash-thinking-exp \n",
      "- supports  ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "\n",
      "models/gemini-2.0-flash-thinking-exp-1219 \n",
      "- supports  ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "\n",
      "models/gemini-2.5-flash-preview-tts \n",
      "- supports  ['countTokens', 'generateContent']\n",
      "\n",
      "models/gemini-2.5-pro-preview-tts \n",
      "- supports  ['countTokens', 'generateContent']\n",
      "\n",
      "models/learnlm-2.0-flash-experimental \n",
      "- supports  ['generateContent', 'countTokens']\n",
      "\n",
      "models/gemma-3-1b-it \n",
      "- supports  ['generateContent', 'countTokens']\n",
      "\n",
      "models/gemma-3-4b-it \n",
      "- supports  ['generateContent', 'countTokens']\n",
      "\n",
      "models/gemma-3-12b-it \n",
      "- supports  ['generateContent', 'countTokens']\n",
      "\n",
      "models/gemma-3-27b-it \n",
      "- supports  ['generateContent', 'countTokens']\n",
      "\n",
      "models/gemma-3n-e4b-it \n",
      "- supports  ['generateContent', 'countTokens']\n",
      "\n",
      "models/gemma-3n-e2b-it \n",
      "- supports  ['generateContent', 'countTokens']\n",
      "\n",
      "models/gemini-flash-latest \n",
      "- supports  ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "\n",
      "models/gemini-flash-lite-latest \n",
      "- supports  ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "\n",
      "models/gemini-pro-latest \n",
      "- supports  ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "\n",
      "models/gemini-2.5-flash-lite \n",
      "- supports  ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "\n",
      "models/gemini-2.5-flash-image-preview \n",
      "- supports  ['generateContent', 'countTokens']\n",
      "\n",
      "models/gemini-2.5-flash-image \n",
      "- supports  ['generateContent', 'countTokens']\n",
      "\n",
      "models/gemini-2.5-flash-preview-09-2025 \n",
      "- supports  ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "\n",
      "models/gemini-2.5-flash-lite-preview-09-2025 \n",
      "- supports  ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "\n",
      "models/gemini-robotics-er-1.5-preview \n",
      "- supports  ['generateContent', 'countTokens']\n",
      "\n",
      "models/embedding-001 \n",
      "- supports  ['embedContent']\n",
      "\n",
      "models/text-embedding-004 \n",
      "- supports  ['embedContent']\n",
      "\n",
      "models/gemini-embedding-exp-03-07 \n",
      "- supports  ['embedContent', 'countTextTokens', 'countTokens']\n",
      "\n",
      "models/gemini-embedding-exp \n",
      "- supports  ['embedContent', 'countTextTokens', 'countTokens']\n",
      "\n",
      "models/gemini-embedding-001 \n",
      "- supports  ['embedContent', 'countTextTokens', 'countTokens', 'asyncBatchEmbedContent']\n",
      "\n",
      "models/aqa \n",
      "- supports  ['generateAnswer']\n",
      "\n",
      "models/imagen-3.0-generate-002 \n",
      "- supports  ['predict']\n",
      "\n",
      "models/imagen-4.0-generate-preview-06-06 \n",
      "- supports  ['predict']\n",
      "\n",
      "models/imagen-4.0-ultra-generate-preview-06-06 \n",
      "- supports  ['predict']\n",
      "\n",
      "models/imagen-4.0-generate-001 \n",
      "- supports  ['predict']\n",
      "\n",
      "models/imagen-4.0-ultra-generate-001 \n",
      "- supports  ['predict']\n",
      "\n",
      "models/imagen-4.0-fast-generate-001 \n",
      "- supports  ['predict']\n",
      "\n",
      "models/veo-2.0-generate-001 \n",
      "- supports  ['predictLongRunning']\n",
      "\n",
      "models/veo-3.0-generate-preview \n",
      "- supports  ['predictLongRunning']\n",
      "\n",
      "models/veo-3.0-fast-generate-preview \n",
      "- supports  ['predictLongRunning']\n",
      "\n",
      "models/veo-3.0-generate-001 \n",
      "- supports  ['predictLongRunning']\n",
      "\n",
      "models/veo-3.0-fast-generate-001 \n",
      "- supports  ['predictLongRunning']\n",
      "\n",
      "models/gemini-2.5-flash-preview-native-audio-dialog \n",
      "- supports  ['countTokens', 'bidiGenerateContent']\n",
      "\n",
      "models/gemini-2.5-flash-exp-native-audio-thinking-dialog \n",
      "- supports  ['countTokens', 'bidiGenerateContent']\n",
      "\n",
      "models/gemini-2.0-flash-live-001 \n",
      "- supports  ['bidiGenerateContent', 'countTokens']\n",
      "\n",
      "models/gemini-live-2.5-flash-preview \n",
      "- supports  ['bidiGenerateContent', 'countTokens']\n",
      "\n",
      "models/gemini-2.5-flash-live-preview \n",
      "- supports  ['bidiGenerateContent', 'countTokens']\n",
      "\n",
      "models/gemini-2.5-flash-native-audio-latest \n",
      "- supports  ['countTokens', 'bidiGenerateContent']\n",
      "\n",
      "models/gemini-2.5-flash-native-audio-preview-09-2025 \n",
      "- supports  ['countTokens', 'bidiGenerateContent']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = genai.list_models()\n",
    "for m in models:\n",
    "    print(m.name, \"\\n- supports \", m.supported_generation_methods)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04583976",
   "metadata": {},
   "source": [
    "## ✍️ Basic Text Generation\n",
    "We’ll use `gemini-2.5-flash` for a simple text generation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80f3ccd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A fiery blush, a golden gleam,\n",
      "They dance and fall, a vibrant dream.\n",
      "From branches high, a gentle flight,\n",
      "Then carpet earth in russet light.\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "\n",
    "response = model.generate_content(\"Write a short poem about autumn leaves.\")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154d8d7c",
   "metadata": {},
   "source": [
    "## 🎛 Playing with Parameters\n",
    "Gemini supports multiple parameters:\n",
    "- **temperature** → randomness (0 = deterministic, 1 = creative)  \n",
    "- **top_p** → nucleus sampling, picks top p probability tokens\n",
    "- **top_k** → how many tokens to sample from\n",
    "- **max_output_tokens** → maximum response token length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7071e78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No text returned. Finish reason: FinishReason.MAX_TOKENS\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain black holes to a 10-year-old.\"\n",
    "\n",
    "response = model.generate_content(\n",
    "    prompt,\n",
    "    generation_config={\n",
    "        \"temperature\": 0.9,\n",
    "        \"top_p\": 0.8,\n",
    "        \"top_k\": 40,\n",
    "        \"max_output_tokens\": 256 # Try increasing the limit\n",
    "    }\n",
    ")\n",
    "# print(response.text)\n",
    "\n",
    "if response.candidates and response.candidates[0].content.parts:\n",
    "    print(response.candidates[0].content.parts[0].text)\n",
    "else:\n",
    "    print(\"No text returned. Finish reason:\", response.candidates[0].finish_reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf57b6e",
   "metadata": {},
   "source": [
    "## 📦 JSON Structured Output\n",
    "We can request structured responses (useful for apps!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f83444e",
   "metadata": {},
   "source": [
    "### 1. Without Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50f0ccdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"state\": \"Maharashtra\",\n",
      "    \"capital\": \"Mumbai\"\n",
      "  },\n",
      "  {\n",
      "    \"state\": \"Karnataka\",\n",
      "    \"capital\": \"Bengaluru\"\n",
      "  },\n",
      "  {\n",
      "    \"state\": \"Rajasthan\",\n",
      "    \"capital\": \"Jaipur\"\n",
      "  }\n",
      "]\n",
      "<class 'str'>\n",
      "[{'state': 'Maharashtra', 'capital': 'Mumbai'}, {'state': 'Karnataka', 'capital': 'Bengaluru'}, {'state': 'Rajasthan', 'capital': 'Jaipur'}]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Return 3 Indian states and their capitals in JSON format.\"\n",
    "\n",
    "response = model.generate_content(\n",
    "    prompt,\n",
    "    generation_config={\n",
    "        \"response_mime_type\": \"application/json\"\n",
    "    }\n",
    ")\n",
    "print(response.text)\n",
    "print(type(response.text))\n",
    "\n",
    "json_response = json.loads(response.text)\n",
    "print(json_response)\n",
    "print(type(json_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ae89be",
   "metadata": {},
   "source": [
    "### 2. With Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41811c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class StateCapital(BaseModel):\n",
    "    state: str\n",
    "    capital: str\n",
    "    food: str\n",
    "\n",
    "class StatesResponse(BaseModel):\n",
    "    states: List[StateCapital]\n",
    "\n",
    "    def to_gemini_schema(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Convert this Pydantic model's JSON Schema into Gemini's\n",
    "        response_schema (OpenAPI-subset) format with all $ref's\n",
    "        dereferenced and only supported fields preserved.\n",
    "        \"\"\"\n",
    "        raw = self.model_json_schema()\n",
    "        return self._convert_schema(raw, root=raw)\n",
    "\n",
    "    _ALLOWED_KEYS = {\n",
    "        \"type\", \"format\", \"properties\", \"required\", \"items\", \"enum\",\n",
    "        \"maxItems\", \"minItems\", \"maximum\", \"minimum\", \"nullable\", \"anyOf\"\n",
    "    }\n",
    "\n",
    "    def _resolve_ref(self, ref: str, root: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Resolve a local JSON Pointer like '#/$defs/Foo' or '#/definitions/Foo'.\"\"\"\n",
    "        if not ref.startswith(\"#/\"):\n",
    "            return {}\n",
    "        node: Dict[str, Any] = root\n",
    "        for part in ref[2:].split(\"/\"):\n",
    "            node = node.get(part, {})\n",
    "        return node\n",
    "\n",
    "    def _strip_unsupported(self, schema: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Remove fields the Gemini response schema ignores.\"\"\"\n",
    "        return {k: v for k, v in schema.items() if k in self._ALLOWED_KEYS}\n",
    "\n",
    "    # CORE CONVERSION LOGIC\n",
    "    def _convert_schema(self, node: Dict[str, Any], root: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        # 1) Resolve $ref if present\n",
    "        if \"$ref\" in node:\n",
    "            node = self._resolve_ref(node[\"$ref\"], root)\n",
    "\n",
    "        # 2) Handle Optional/union encoded as anyOf[..., {\"type\": \"null\"}]\n",
    "        if \"anyOf\" in node:\n",
    "            variants = node[\"anyOf\"]\n",
    "            nullable = any(v.get(\"type\") == \"null\" for v in variants)\n",
    "            non_null = [v for v in variants if v.get(\"type\") != \"null\"]\n",
    "            if len(non_null) == 1:\n",
    "                base = self._convert_schema(non_null[0], root)\n",
    "                if nullable:\n",
    "                    base[\"nullable\"] = True\n",
    "                return base\n",
    "            # General union: keep anyOf (Gemini supports anyOf)\n",
    "            return {\"anyOf\": [self._convert_schema(v, root) for v in non_null]}\n",
    "\n",
    "        typ = node.get(\"type\")\n",
    "\n",
    "        # 3) Objects\n",
    "        if typ == \"object\" or \"properties\" in node:\n",
    "            out: Dict[str, Any] = {\"type\": \"object\", \"properties\": {}}\n",
    "            if \"required\" in node:\n",
    "                out[\"required\"] = list(node[\"required\"])\n",
    "            for name, prop in (node.get(\"properties\") or {}).items():\n",
    "                out[\"properties\"][name] = self._convert_schema(prop, root)\n",
    "            # nullable object if Pydantic put it at this level (rare)\n",
    "            if node.get(\"nullable\") is True:\n",
    "                out[\"nullable\"] = True\n",
    "            return self._strip_unsupported(out)\n",
    "\n",
    "        # 4) Arrays\n",
    "        if typ == \"array\":\n",
    "            items = self._convert_schema(node.get(\"items\", {}), root)\n",
    "            arr: Dict[str, Any] = {\"type\": \"array\", \"items\": items}\n",
    "            if \"minItems\" in node:\n",
    "                arr[\"minItems\"] = node[\"minItems\"]\n",
    "            if \"maxItems\" in node:\n",
    "                arr[\"maxItems\"] = node[\"maxItems\"]\n",
    "            return self._strip_unsupported(arr)\n",
    "\n",
    "        # 5) Primitives\n",
    "        if typ in {\"string\", \"integer\", \"number\", \"boolean\"}:\n",
    "            leaf: Dict[str, Any] = {\"type\": typ}\n",
    "            # carry through supported constraints if present\n",
    "            for k in (\"format\", \"enum\", \"minimum\", \"maximum\"):\n",
    "                if k in node:\n",
    "                    leaf[k] = node[k]\n",
    "            if node.get(\"nullable\") is True:\n",
    "                leaf[\"nullable\"] = True\n",
    "            return self._strip_unsupported(leaf)\n",
    "\n",
    "        # 6) Fallback (treat as string)\n",
    "        return {\"type\": \"string\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb5e667c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pydantic schema:\n",
      "{'$defs': {'StateCapital': {'properties': {'capital': {'title': 'Capital',\n",
      "                                                       'type': 'string'},\n",
      "                                           'food': {'title': 'Food',\n",
      "                                                    'type': 'string'},\n",
      "                                           'state': {'title': 'State',\n",
      "                                                     'type': 'string'}},\n",
      "                            'required': ['state', 'capital', 'food'],\n",
      "                            'title': 'StateCapital',\n",
      "                            'type': 'object'}},\n",
      " 'properties': {'states': {'items': {'$ref': '#/$defs/StateCapital'},\n",
      "                           'title': 'States',\n",
      "                           'type': 'array'}},\n",
      " 'required': ['states'],\n",
      " 'title': 'StatesResponse',\n",
      " 'type': 'object'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gemini-compatible schema:\n",
      "{'properties': {'states': {'items': {'properties': {'capital': {'type': 'string'},\n",
      "                                                    'food': {'type': 'string'},\n",
      "                                                    'state': {'type': 'string'}},\n",
      "                                     'required': ['state', 'capital', 'food'],\n",
      "                                     'type': 'object'},\n",
      "                           'type': 'array'}},\n",
      " 'required': ['states'],\n",
      " 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "response_format = StatesResponse(states=[StateCapital(state=\"Tamil Nadu\", capital=\"Chennai\", food=\"Dosa\"),])\n",
    "\n",
    "# Default pydantic schema\n",
    "print(\"Pydantic schema:\")\n",
    "pprint(response_format.model_json_schema())\n",
    "print(\"-\"*100)\n",
    "\n",
    "# Gemini-compatible schema\n",
    "print(\"Gemini-compatible schema:\")\n",
    "pprint(response_format.to_gemini_schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0326cb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'states': [{'state': 'Maharashtra', 'capital': 'Mumbai', 'food': 'Vada Pav'}, {'state': 'Rajasthan', 'capital': 'Jaipur', 'food': 'Dal Bati Churma'}, {'state': 'Karnataka', 'capital': 'Bengaluru', 'food': 'Bisi Bele Bath'}]}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Return 3 Indian states and their capitals in JSON format.\"\n",
    "\n",
    "response = model.generate_content(\n",
    "    prompt,\n",
    "    generation_config={\n",
    "        \"response_mime_type\": \"application/json\",\n",
    "        \"response_schema\": response_format.to_gemini_schema()\n",
    "    }\n",
    ")\n",
    "\n",
    "print(json.loads(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a216c3",
   "metadata": {},
   "source": [
    "## 💬 Multi-Turn Chat\n",
    "Gemini supports chat-like interactions where history is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1367083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Hello! I am a large language model, trained by Google.\n",
      "\n",
      "I'm here to provide information, answer your questions, generate creative text, and assist you with various tasks through conversation.\n",
      "\n",
      "How can I help you today?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Bot: Imagine a regular computer as a light switch that can be either **ON (1)** or **OFF (0)**. It processes information by flipping these switches one by one.\n",
      "\n",
      "Quantum computing is like having a light switch that can be **ON**, **OFF**, and also **ON and OFF at the same time** (in a blurry, \"maybe\" state). And what's more, these \"maybe\" switches can influence each other in strange ways.\n",
      "\n",
      "Let's break it down into three key ideas:\n",
      "\n",
      "1.  **Qubits (Quantum Bits): The \"Maybe\" Switch**\n",
      "    *   **Classical Bit:** Is either a 0 or a 1. Like a coin lying flat, showing either heads or tails.\n",
      "    *   **Quantum Qubit:** Can be a 0, a 1, *or both at the same time* (this is called **superposition**). Think of a coin spinning in the air – it's neither heads nor tails until it lands. A qubit is like that spinning coin, holding both possibilities simultaneously.\n",
      "    *   **Why this matters:** If you have just a few qubits, they can store and process exponentially more information than the same number of classical bits because each qubit can represent many possibilities at once.\n",
      "\n",
      "2.  **Superposition: All Paths at Once**\n",
      "    *   Because a qubit can be both 0 and 1 at the same time, a quantum computer can explore many different solutions to a problem *simultaneously*.\n",
      "    *   Imagine you're trying to find the shortest path through a maze. A classical computer tries one path, then if it fails, backs up and tries another. A quantum computer, due to superposition, can essentially \"try all paths at once.\"\n",
      "\n",
      "3.  **Entanglement: Spooky Connection**\n",
      "    *   This is where it gets really weird. When qubits are \"entangled,\" they become linked in a way that their fates are intertwined, no matter how far apart they are.\n",
      "    *   If you measure one entangled qubit, you instantly know something about the other, even if it's across the galaxy.\n",
      "    *   **Why this matters:** This allows qubits to work together in a highly coordinated way, sharing information and amplifying the correct answer while cancelling out the incorrect ones. It's like having many maze explorers who are all telepathically linked and can instantly tell each other which paths are dead ends.\n",
      "\n",
      "**So, how does it actually work?**\n",
      "\n",
      "A quantum computer sets up these qubits in superposition and entanglement, performs calculations by manipulating these \"maybe\" states, and then, at the end, it \"looks\" at the qubits. When you look, the \"spinning coin\" finally lands on either heads (1) or tails (0).\n",
      "\n",
      "**What is it good for?**\n",
      "\n",
      "It's *not* going to replace your laptop for browsing the web or writing emails. Quantum computers excel at very specific, incredibly complex problems that classical computers simply can't handle efficiently, such as:\n",
      "\n",
      "*   **Drug Discovery:** Simulating molecules to find new medicines.\n",
      "*   **Materials Science:** Designing new materials with specific properties (e.g., superconductors).\n",
      "*   **Cryptography:** Breaking current encryption methods and creating new, ultra-secure ones.\n",
      "*   **Optimization:** Finding the absolute best solution among an enormous number of possibilities (e.g., optimizing logistics, financial modeling).\n",
      "*   **Artificial Intelligence:** Enhancing machine learning algorithms.\n",
      "\n",
      "In short, quantum computing is about harnessing the bizarre rules of the subatomic world to solve problems that are currently impossible for even the most powerful supercomputers. It's still in its early stages, but the potential is enormous!\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Bot: The first question you asked me was:\n",
      "\n",
      "\"Hello, who are you?\"\n"
     ]
    }
   ],
   "source": [
    "chat = model.start_chat(history=[])\n",
    "\n",
    "r1 = chat.send_message(\"Hello, who are you?\")\n",
    "print(\"Bot:\", r1.text)\n",
    "print(\"-\"*100)\n",
    "\n",
    "r2 = chat.send_message(\"Can you explain quantum computing simply?\")\n",
    "print(\"Bot:\", r2.text)\n",
    "print(\"-\"*100)\n",
    "\n",
    "r3 = chat.send_message(\"What was the first question I asked you?\")\n",
    "print(\"Bot:\", r3.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b2026bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[parts {\n",
       "   text: \"Hello, who are you?\"\n",
       " }\n",
       " role: \"user\",\n",
       " parts {\n",
       "   text: \"Hello! I am a large language model, trained by Google.\\n\\nI\\'m here to provide information, answer your questions, generate creative text, and assist you with various tasks through conversation.\\n\\nHow can I help you today?\"\n",
       " }\n",
       " role: \"model\",\n",
       " parts {\n",
       "   text: \"Can you explain quantum computing simply?\"\n",
       " }\n",
       " role: \"user\",\n",
       " parts {\n",
       "   text: \"Imagine a regular computer as a light switch that can be either **ON (1)** or **OFF (0)**. It processes information by flipping these switches one by one.\\n\\nQuantum computing is like having a light switch that can be **ON**, **OFF**, and also **ON and OFF at the same time** (in a blurry, \\\"maybe\\\" state). And what\\'s more, these \\\"maybe\\\" switches can influence each other in strange ways.\\n\\nLet\\'s break it down into three key ideas:\\n\\n1.  **Qubits (Quantum Bits): The \\\"Maybe\\\" Switch**\\n    *   **Classical Bit:** Is either a 0 or a 1. Like a coin lying flat, showing either heads or tails.\\n    *   **Quantum Qubit:** Can be a 0, a 1, *or both at the same time* (this is called **superposition**). Think of a coin spinning in the air – it\\'s neither heads nor tails until it lands. A qubit is like that spinning coin, holding both possibilities simultaneously.\\n    *   **Why this matters:** If you have just a few qubits, they can store and process exponentially more information than the same number of classical bits because each qubit can represent many possibilities at once.\\n\\n2.  **Superposition: All Paths at Once**\\n    *   Because a qubit can be both 0 and 1 at the same time, a quantum computer can explore many different solutions to a problem *simultaneously*.\\n    *   Imagine you\\'re trying to find the shortest path through a maze. A classical computer tries one path, then if it fails, backs up and tries another. A quantum computer, due to superposition, can essentially \\\"try all paths at once.\\\"\\n\\n3.  **Entanglement: Spooky Connection**\\n    *   This is where it gets really weird. When qubits are \\\"entangled,\\\" they become linked in a way that their fates are intertwined, no matter how far apart they are.\\n    *   If you measure one entangled qubit, you instantly know something about the other, even if it\\'s across the galaxy.\\n    *   **Why this matters:** This allows qubits to work together in a highly coordinated way, sharing information and amplifying the correct answer while cancelling out the incorrect ones. It\\'s like having many maze explorers who are all telepathically linked and can instantly tell each other which paths are dead ends.\\n\\n**So, how does it actually work?**\\n\\nA quantum computer sets up these qubits in superposition and entanglement, performs calculations by manipulating these \\\"maybe\\\" states, and then, at the end, it \\\"looks\\\" at the qubits. When you look, the \\\"spinning coin\\\" finally lands on either heads (1) or tails (0).\\n\\n**What is it good for?**\\n\\nIt\\'s *not* going to replace your laptop for browsing the web or writing emails. Quantum computers excel at very specific, incredibly complex problems that classical computers simply can\\'t handle efficiently, such as:\\n\\n*   **Drug Discovery:** Simulating molecules to find new medicines.\\n*   **Materials Science:** Designing new materials with specific properties (e.g., superconductors).\\n*   **Cryptography:** Breaking current encryption methods and creating new, ultra-secure ones.\\n*   **Optimization:** Finding the absolute best solution among an enormous number of possibilities (e.g., optimizing logistics, financial modeling).\\n*   **Artificial Intelligence:** Enhancing machine learning algorithms.\\n\\nIn short, quantum computing is about harnessing the bizarre rules of the subatomic world to solve problems that are currently impossible for even the most powerful supercomputers. It\\'s still in its early stages, but the potential is enormous!\"\n",
       " }\n",
       " role: \"model\",\n",
       " parts {\n",
       "   text: \"What was the first question I asked you?\"\n",
       " }\n",
       " role: \"user\",\n",
       " parts {\n",
       "   text: \"The first question you asked me was:\\n\\n\\\"Hello, who are you?\\\"\"\n",
       " }\n",
       " role: \"model\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bf41cf",
   "metadata": {},
   "source": [
    "## 🛡 Safety Settings\n",
    "We can adjust safety thresholds for categories like hate speech, dangerous content, etc.\n",
    "\n",
    "Some supported categories are:\n",
    "- `HARM_CATEGORY_HARASSMENT`\n",
    "- `HARM_CATEGORY_HATE_SPEECH`\n",
    "- `HARM_CATEGORY_SEXUALLY_EXPLICIT`\n",
    "- `HARM_CATEGORY_DANGEROUS_CONTENT`\n",
    "- `HARM_CATEGORY_CIVIC_INTEGRITY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5f66834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a fantastic, classic chocolate cake recipe that's incredibly moist, rich, and perfect for any occasion. It's often called \"Wacky Cake\" or \"Depression-Era Cake\" because it uses oil instead of butter and no eggs, yet still produces a phenomenal result. I'll include a simple, delicious chocolate buttercream frosting to go with it.\n",
      "\n",
      "---\n",
      "\n",
      "## The Best Moist Chocolate Cake with Chocolate Buttercream\n",
      "\n",
      "This cake is surprisingly easy to make and yields a deeply chocolatey, super moist crumb. The hot coffee isn't for flavor (you won't taste it!), but it helps \"bloom\" the cocoa powder, intensifying the chocolate flavor and contributing to the cake's tender texture.\n",
      "\n",
      "**Yields:** One 9x13 inch sheet cake OR two 8 or 9-inch round cake layers (for a layer cake)\n",
      "**Prep time:** 20 minutes\n",
      "**Cook time:** 30-35 minutes (for rounds), 35-40 minutes (for sheet cake)\n",
      "\n",
      "---\n",
      "\n",
      "### **Part 1: The Chocolate Cake**\n",
      "\n",
      "**Ingredients:**\n",
      "\n",
      "*   2 cups (260g) All-Purpose Flour\n",
      "*   2 cups (400g) Granulated Sugar\n",
      "*   ¾ cup (75g) Unsweetened Cocoa Powder (Dutch-processed for a darker, less acidic flavor, but natural works too)\n",
      "*   2 teaspoons Baking Soda\n",
      "*   1 teaspoon Baking Powder\n",
      "*   1 teaspoon Salt\n",
      "*   1 cup (240ml) Milk (whole milk or buttermilk work best)\n",
      "*   ½ cup (120ml) Vegetable Oil (or canola oil)\n",
      "*   2 large Eggs\n",
      "*   2 teaspoons Vanilla Extract\n",
      "*   1 cup (240ml) Hot Brewed Coffee (or hot water for a less intense chocolate flavor)\n",
      "\n",
      "**Equipment:**\n",
      "\n",
      "*   9x13 inch baking pan, or two 8 or 9-inch round cake pans\n",
      "*   Large mixing bowl\n",
      "*   Whisk\n",
      "*   Rubber spatula\n",
      "*   Measuring cups and spoons\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "1.  **Preheat & Prep:** Preheat your oven to 350°F (175°C). Grease and flour your chosen cake pans. If using round pans, line the bottoms with parchment paper for easy removal.\n",
      "2.  **Combine Dry Ingredients:** In a large mixing bowl, whisk together the flour, granulated sugar, cocoa powder, baking soda, baking powder, and salt. Make sure there are no lumps of cocoa powder.\n",
      "3.  **Add Wet Ingredients (except hot coffee):** Add the milk, vegetable oil, eggs, and vanilla extract to the dry ingredients. Beat with an electric mixer on medium speed (or whisk vigorously by hand) for about 2 minutes, until well combined and smooth. Scrape down the sides of the bowl as needed.\n",
      "4.  **Add Hot Coffee:** Carefully pour the hot brewed coffee (or hot water) into the batter. Mix on low speed (or gently with a whisk) just until combined. The batter will be thin – that's normal! Do not overmix.\n",
      "5.  **Bake:** Pour the batter evenly into your prepared pan(s).\n",
      "    *   For **two 8 or 9-inch round pans:** Bake for 30-35 minutes.\n",
      "    *   For a **9x13 inch sheet cake:** Bake for 35-40 minutes.\n",
      "    *   The cake is done when a wooden skewer or toothpick inserted into the center comes out with moist crumbs, not wet batter.\n",
      "6.  **Cool:** Let the cake(s) cool in the pans for about 10-15 minutes before inverting them onto a wire rack to cool completely. **It is crucial for the cake to be completely cool before frosting.**\n",
      "\n",
      "---\n",
      "\n",
      "### **Part 2: Chocolate Buttercream Frosting**\n",
      "\n",
      "**Ingredients:**\n",
      "\n",
      "*   1 cup (226g) Unsalted Butter, softened (2 sticks)\n",
      "*   ¾ cup (75g) Unsweetened Cocoa Powder\n",
      "*   4-5 cups (480-600g) Powdered Sugar (Confectioners' Sugar), sifted\n",
      "*   ¼ teaspoon Salt\n",
      "*   ⅓ cup (80ml) Milk (whole milk or heavy cream for a richer frosting)\n",
      "*   1 teaspoon Vanilla Extract\n",
      "\n",
      "**Equipment:**\n",
      "\n",
      "*   Electric mixer (stand mixer or hand mixer)\n",
      "*   Large mixing bowl\n",
      "*   Rubber spatula\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "1.  **Beat Butter:** In a large bowl with an electric mixer, beat the softened butter on medium speed until light and fluffy (about 2-3 minutes).\n",
      "2.  **Add Cocoa & Salt:** Add the cocoa powder and salt to the butter. Beat on low speed until combined, then increase to medium and beat until fully incorporated.\n",
      "3.  **Add Powdered Sugar & Milk:** Gradually add the sifted powdered sugar, alternating with the milk, starting and ending with powdered sugar. Beat on low speed until mostly combined, then increase to medium-high and beat until light and fluffy. Add more powdered sugar if you want a thicker frosting, or more milk (1 teaspoon at a time) if you want it thinner.\n",
      "4.  **Add Vanilla:** Stir in the vanilla extract until just combined.\n",
      "5.  **Beat Until Fluffy:** Continue to beat the frosting on medium-high for another 2-3 minutes until it's light, fluffy, and spreadable.\n",
      "\n",
      "---\n",
      "\n",
      "### **Part 3: Assembly**\n",
      "\n",
      "1.  **Level (Optional):** If making a layer cake, use a serrated knife to level the tops of your cooled cake layers so they are flat.\n",
      "2.  **Crumb Coat (Optional but Recommended for Layer Cakes):** Place one cake layer on your serving plate or stand. Spread a thin layer of frosting over the top and sides. This \"crumb coat\" traps any loose crumbs. Refrigerate for 15-20 minutes to set.\n",
      "3.  **Final Frosting:** Once the crumb coat is set (or if you skipped it), spread a generous amount of frosting between the layers, then cover the top and sides of the cake with the remaining frosting.\n",
      "4.  **Decorate:** Decorate as desired with sprinkles, chocolate shavings, or a piping bag.\n",
      "\n",
      "---\n",
      "\n",
      "Enjoy your delicious homemade chocolate cake!\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\n",
    "    \"Give me a recipe for chocolate cake\",\n",
    "    safety_settings=[\n",
    "        {\n",
    "            \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "            \"threshold\": \"BLOCK_ONLY_HIGH\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fbc645",
   "metadata": {},
   "source": [
    "## 🧠 Reasoning Mode (Experimental)\n",
    "Some Gemini models (e.g., `gemini-2.0-flash-thinking-exp-1219`) support reasoning with intermediate steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c98e30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's how to solve this problem:\n",
      "\n",
      "1.  **Calculate Train 1's head start:**\n",
      "    Train 1 leaves at 2 PM and Train 2 leaves at 3 PM. This means Train 1 travels for 1 hour before Train 2 even starts.\n",
      "    Distance covered by Train 1 in that hour = Speed × Time = 60 km/h × 1 h = 60 km.\n",
      "    So, at 3 PM, Train 1 is 60 km ahead of Train 2.\n",
      "\n",
      "2.  **Determine the relative speed:**\n",
      "    Both trains are traveling in the same direction, but Train 2 is faster. The rate at which Train 2 closes the gap on Train 1 is their difference in speed.\n",
      "    Relative speed = Speed of Train 2 - Speed of Train 1\n",
      "    Relative speed = 90 km/h - 60 km/h = 30 km/h.\n",
      "    This means Train 2 gains 30 km on Train 1 every hour.\n",
      "\n",
      "3.  **Calculate the time it takes for Train 2 to catch up:**\n",
      "    Train 2 needs to close a 60 km gap at a relative speed of 30 km/h.\n",
      "    Time = Distance / Relative Speed\n",
      "    Time = 60 km / 30 km/h = 2 hours.\n",
      "\n",
      "4.  **Find the meeting time:**\n",
      "    Train 2 starts at 3 PM and takes 2 hours to catch up.\n",
      "    Meeting time = 3 PM + 2 hours = **5 PM**.\n",
      "\n",
      "**To verify:**\n",
      "*   At 5 PM, Train 1 has been traveling for 3 hours (from 2 PM). Distance = 60 km/h * 3 h = 180 km.\n",
      "*   At 5 PM, Train 2 has been traveling for 2 hours (from 3 PM). Distance = 90 km/h * 2 h = 180 km.\n",
      "They both cover 180 km, so they meet at 5 PM.\n"
     ]
    }
   ],
   "source": [
    "reasoning_model = genai.GenerativeModel(\"gemini-2.0-flash-thinking-exp-1219\")\n",
    "\n",
    "resp = reasoning_model.generate_content(\n",
    "    \"Solve: If a train leaves at 2 PM at 60 km/h and another at 3 PM at 90 km/h, when do they meet?\",\n",
    "    generation_config={\n",
    "        \"temperature\": 0.2\n",
    "    }\n",
    ")\n",
    "print(resp.text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
