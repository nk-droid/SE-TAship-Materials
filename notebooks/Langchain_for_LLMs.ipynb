{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP1xdra9-eUI"
      },
      "source": [
        "## How to inference Hugging Face models using Langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "DG5xDpIp0IBL"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
            "[notice] To update, run: C:\\Users\\HP\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# install the necessay packages\n",
        "! pip -q install langchain langchain_community langchain_google_genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "4Yx_5oFk0Saz"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmyLHydR0Tds",
        "outputId": "2a70fd1e-1ad9-4541-807c-4f6b49c7e63b"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# prompt template for evaluating answers\n",
        "template = \"\"\"Evaluate the following question and check whether the given answer is correct or not.\n",
        "\n",
        "Question: {question}\n",
        "Answer: {answer}\n",
        "\"\"\"\n",
        "\n",
        "# create a PromptTemplate instance with the defined template and input variables\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"question\", \"answer\"]\n",
        ")\n",
        "\n",
        "# initialize the Hugging Face model endpoint for inference\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "YjuZj4Y11TQ9"
      },
      "outputs": [],
      "source": [
        "# create a processing chain of the prompt and model\n",
        "chain = prompt | model\n",
        "\n",
        "question = \"What is 2+2?\"\n",
        "answer = \"5\"\n",
        "\n",
        "# invoke the chain with the question and answer, retrieving the result\n",
        "result = chain.invoke({\n",
        "    \"question\": f\"{question}\",\n",
        "    \"answer\": f\"{answer}\"\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sY_8QANL4UD6",
        "outputId": "75c0fee7-2c68-4a61-c687-bbf640bae41b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The given answer is **incorrect**.\n",
            "\n",
            "*   **Question:** What is 2+2?\n",
            "*   **Correct Answer:** 4\n",
            "*   **Given Answer:** 5\n",
            "\n",
            "2 + 2 equals 4, not 5.\n"
          ]
        }
      ],
      "source": [
        "print(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9Cz74hR9N74"
      },
      "source": [
        "## Langchain with Pydantic Schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "S5PXbwdW4UfN"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict, Optional # for complex outputs\n",
        "from pydantic import Field, BaseModel\n",
        "from langchain.output_parsers import PydanticOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "_Z_aiLRy4jq1"
      },
      "outputs": [],
      "source": [
        "class EvaluationSchema(BaseModel):\n",
        "    # This sets the default value as the text provided.\n",
        "    # correct: str = Field(\"Check if the provided answer is correct or not - TRUE or FALSE.\")\n",
        "\n",
        "    correct: bool = Field(description=\"Check if the provided answer is correct or not.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "SNhocTuD5NRH"
      },
      "outputs": [],
      "source": [
        "class QuestionEvaluator:\n",
        "    def __init__(self) -> None:\n",
        "\n",
        "        # intialize the prompt template for evaluating questions and answers\n",
        "        self.template = \"\"\"\n",
        "        Evaluate the following question and check whether the given answer is correct or not.\n",
        "\n",
        "        Question: {question}\n",
        "        Answer: {answer}\n",
        "\n",
        "        {format_instructions}\n",
        "        \"\"\"\n",
        "\n",
        "        # initialize the Hugging Face model endpoint for evaluation\n",
        "        self.model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
        "\n",
        "        # initialize a parser for output\n",
        "        self.parser = PydanticOutputParser(pydantic_object=EvaluationSchema)\n",
        "\n",
        "    def get_chain(self):\n",
        "\n",
        "        # retrieving format instructions from the parser\n",
        "        format_instructions = self.parser.get_format_instructions()\n",
        "\n",
        "        # creating a PromptTemplate instance with the evaluation template and input variables\n",
        "        prompt = PromptTemplate(\n",
        "            template=self.template,\n",
        "            input_variables=[\n",
        "                \"question\",\n",
        "                \"answer\"\n",
        "            ],\n",
        "            partial_variables={\n",
        "                \"format_instructions\": format_instructions\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # creating a processing chain that combines prompt, model, and parser\n",
        "        chain = prompt | self.model | self.parser\n",
        "\n",
        "        return chain\n",
        "\n",
        "    def invoke_chain(self, question, answer):\n",
        "\n",
        "        # load the processing chain\n",
        "        chain = self.get_chain()\n",
        "\n",
        "        # invoke the chain\n",
        "        result = chain.invoke({\n",
        "            \"question\": f\"{question}\",\n",
        "            \"answer\": f\"{answer}\"\n",
        "        })\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2F-a5Xi7kXF",
        "outputId": "0136f8a5-c2e0-4cb3-c158-d057ff4665e9"
      },
      "outputs": [],
      "source": [
        "question = \"What is 2+2?\"\n",
        "answer = \"5\"\n",
        "\n",
        "# create instance of QuestionEvaluator\n",
        "evaluator = QuestionEvaluator()\n",
        "\n",
        "# invoke the evaluation chain with the specified question and answer\n",
        "result = evaluator.invoke_chain(\n",
        "    question=question,\n",
        "    answer=answer\n",
        ").model_dump()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWURf9h2m-r-",
        "outputId": "8e97b678-3351-4885-b946-8385214fea92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'correct': False}\n"
          ]
        }
      ],
      "source": [
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
