{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class SearchResult(BaseModel):\n",
    "    content: str\n",
    "    metadata: Dict = None\n",
    "\n",
    "class ExpandedQueries(BaseModel):\n",
    "    queries: List[str] = Field(..., description=\"List of queries\")\n",
    "\n",
    "class Response(BaseModel):\n",
    "    response: str = Field(..., description=\"Summary from relevant docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_expansion_promt = \"\"\"\n",
    "Given the following user question, generate three different versions of the query \n",
    "that could help retrieve relevant information. Make them diverse but related to the \n",
    "original question.\n",
    "\n",
    "Original question: {question}\n",
    "\n",
    "Return the result in the following format:\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "final_prompt = \"\"\"\n",
    "Answer the question based on the following context and chat history. Generate the summary using relevant context to answer the question.\n",
    "If you cannot answer the question based on the context, say so.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Return the result in the following format:\n",
    "{format_instructions}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "\n",
    "class RAGPipeline:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "        self.embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "        self.setup_components()\n",
    "\n",
    "    def setup_components(self):\n",
    "        \"\"\"Initialize all RAG components\"\"\"\n",
    "        # Text splitter for document processing\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200\n",
    "        )\n",
    "        \n",
    "        # Initialize empty vector store\n",
    "        self.vector_store = None\n",
    "        self.bm25_retriever = None\n",
    "        self.ensemble_retriever = None\n",
    "        \n",
    "        # Setup prompt templates\n",
    "        self.query_parser = PydanticOutputParser(pydantic_object=ExpandedQueries)\n",
    "        self.query_expansion_prompt = PromptTemplate.from_template(\n",
    "            template=query_expansion_promt,\n",
    "            input=[\"question\"],\n",
    "            partial_variables={'format_instructions': self.query_parser.get_format_instructions()}\n",
    "        )\n",
    "        \n",
    "        self.final_response_parser = PydanticOutputParser(pydantic_object=Response)\n",
    "        self.final_prompt = PromptTemplate.from_template(\n",
    "            template=final_prompt,\n",
    "            input=[\"context\", \"question\"],\n",
    "            partial_variables={'format_instructions': self.final_response_parser.get_format_instructions()}\n",
    "        )\n",
    "\n",
    "    def ingest_documents(self, documents: List[str], metadata_list: List[Dict] = None):\n",
    "        \"\"\"Process and store documents in vector store and BM25\"\"\"\n",
    "        if metadata_list is None:\n",
    "            metadata_list = [{}] * len(documents)\n",
    "            \n",
    "        # Process documents\n",
    "        docs = [\n",
    "            Document(page_content=doc, metadata=meta)\n",
    "            for doc, meta in zip(documents, metadata_list)\n",
    "        ]\n",
    "        splits = self.text_splitter.split_documents(docs)\n",
    "        \n",
    "        # Create ensemble retriever\n",
    "        self.vector_store = FAISS.from_documents(splits, self.embeddings)\n",
    "        self.bm25_retriever = BM25Retriever.from_documents(splits)\n",
    "        self.ensemble_retriever = EnsembleRetriever(\n",
    "            retrievers=[\n",
    "                self.vector_store.as_retriever(search_kwargs={\"k\": 10}),\n",
    "                self.bm25_retriever\n",
    "            ],\n",
    "            weights=[0.7, 0.3]\n",
    "        )\n",
    "\n",
    "    def expand_query(self, question: str) -> List[str]:\n",
    "        \"\"\"Generate multiple versions of the query using LLM\"\"\"\n",
    "        chain = self.query_expansion_prompt | self.llm | self.query_parser\n",
    "        result = chain.invoke({\n",
    "            \"question\": question\n",
    "        }).model_dump()\n",
    "\n",
    "        return result[\"queries\"]\n",
    "\n",
    "    def retrieve_and_fuse(self, question: str, n: int) -> List[Document]:\n",
    "        \"\"\"Retrieve documents using multiple queries and fuse results\"\"\"\n",
    "        if not self.ensemble_retriever:\n",
    "            raise ValueError(\"No documents ingested yet. Please call ingest_documents first.\")\n",
    "            \n",
    "        # Expand the query\n",
    "        self.expanded_queries = self.expand_query(question)\n",
    "        all_docs = []\n",
    "        \n",
    "        # Retrieve documents for each query\n",
    "        for query in [question] + self.expanded_queries:\n",
    "            docs = self.ensemble_retriever.invoke(query)\n",
    "            all_docs.extend(docs)\n",
    "            \n",
    "        # Remove duplicates and sort by relevance\n",
    "        seen = set()\n",
    "        unique_docs = []\n",
    "        for doc in all_docs:\n",
    "            if doc.page_content not in seen:\n",
    "                seen.add(doc.page_content)\n",
    "                unique_docs.append(doc)\n",
    "                \n",
    "        return unique_docs[:n]\n",
    "\n",
    "    def process_query(self, question: str, n: int) -> Dict[str, Any]:\n",
    "        \"\"\"Main method to process a query and generate response\"\"\"\n",
    "        # Retrieve relevant documents\n",
    "        relevant_docs = self.retrieve_and_fuse(question, n)\n",
    "        docs = []\n",
    "        for doc in relevant_docs:\n",
    "            docs.append(\n",
    "                SearchResult(\n",
    "                    content=doc.page_content,\n",
    "                    source=doc.metadata.get('source', 'unknown'),\n",
    "                    metadata=doc.metadata\n",
    "                ).model_dump()\n",
    "            )\n",
    "        \n",
    "        # Format context from documents\n",
    "        context = \"\\n\\n\".join(doc.page_content for doc in relevant_docs)\n",
    "        \n",
    "        # Create response chain and invoke it\n",
    "        chain = self.final_prompt | self.llm | self.final_response_parser\n",
    "        response = chain.invoke({\n",
    "            \"context\": context,\n",
    "            \"question\": question\n",
    "        }).model_dump()\n",
    "        \n",
    "        return {\n",
    "            \"response\": response[\"response\"],\n",
    "            \"relevant_docs\": docs,\n",
    "            \"expanded_queries\": self.expanded_queries\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "\n",
    "def load_content(filename) -> str:\n",
    "\troot = \"../assets/documents\"\n",
    "\tfilepath = os.path.join(root, filename)\n",
    "\twith open(filepath, \"r\", encoding='utf-8') as file:\n",
    "\t\tcontent = file.read()\n",
    "\treturn content\n",
    "\n",
    "def get_file_details(filename) -> Dict[str, str]:\n",
    "\troot = \"../assets/documents\"\n",
    "\tfilepath = os.path.join(root, filename)\n",
    "\n",
    "\tist = pytz.timezone(\"Asia/Kolkata\")\n",
    "\tcreation_time_utc = datetime.fromtimestamp(os.path.getctime(filepath))\n",
    "\tcreation_time_ist = creation_time_utc.astimezone(ist).strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "\n",
    "\treturn {\n",
    "\t\t\"filename\": os.path.basename(filepath),\n",
    "\t\t\"creation_time\": creation_time_ist,\n",
    "\t\t\"size_kb\": f\"{round(os.path.getsize(filepath) / 1024, 2)} KB\"\n",
    "\t}\n",
    "\n",
    "# Example usage\n",
    "def main(query: str, n: int = 5) -> Dict[str, Any]:\n",
    "\t# Initialize the pipeline\n",
    "\trag = RAGPipeline()\n",
    "\n",
    "\t# Sample documents\n",
    "\tdocuments = [load_content(filepath) for filepath in os.listdir(\"../assets/documents\")]\n",
    "\tmetadata = [get_file_details(filepath) for filepath in os.listdir(\"../assets/documents\")]\n",
    "\n",
    "\t# Ingest documents\n",
    "\trag.ingest_documents(\n",
    "\t\tdocuments,\n",
    "\t\tmetadata_list=metadata\n",
    "\t)\n",
    "\n",
    "\tresult = rag.process_query(query, n)\n",
    "\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is RAG? What is the importance of vector db in RAG implementation?\"\n",
    "result = main(query, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Explain Retrieval Augmented Generation (RAG) and the role of vector databases.',\n",
       " 'How do vector databases contribute to the effectiveness of RAG systems?',\n",
       " 'RAG architecture: benefits of using vector embeddings and similarity search.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['expanded_queries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': '### Challenges and Considerations\\n\\n- **Latency**: Retrieving documents and generating responses can introduce delays.\\n- **Knowledge Base Maintenance**: The quality of responses depends on the reliability and freshness of the external knowledge source.\\n- **Computational Costs**: Running both retrieval and generation components can be resource-intensive.\\n\\n### Future of RAG\\n\\nWith ongoing advancements in AI, RAG is expected to become more efficient and widely adopted. Future improvements may include better retriever architectures, optimized response generation, and lower latency techniques to ensure seamless real-time applications.',\n",
       "  'metadata': {'filename': 'rag.md',\n",
       "   'creation_time': '2025-02-07 15:19:43 IST',\n",
       "   'size_kb': '2.75 KB'}},\n",
       " {'content': '#### Working Mechanism\\n\\n1. **Query Processing**: The user provides an input query.\\n2. **Document Retrieval**: The retriever fetches the top-k relevant documents from an external knowledge base.\\n3. **Response Generation**: The retrieved documents, along with the query, are fed into the generative model to produce a response.\\n4. **Output Optimization**: The generated response is refined to improve fluency and relevance.\\n\\n#### Advantages of RAG',\n",
       "  'metadata': {'filename': 'rag.md',\n",
       "   'creation_time': '2025-02-07 15:19:43 IST',\n",
       "   'size_kb': '2.75 KB'}},\n",
       " {'content': '#### Advantages of RAG\\n\\n- **Incorporates External Knowledge**: Unlike standalone generative models, RAG can access up-to-date or domain-specific information.\\n- **Reduces Hallucinations**: Since the model relies on retrieved facts, it is less likely to generate incorrect or fabricated content.\\n- **Improves Contextual Accuracy**: The responses are more aligned with user queries, as they are based on factual information.\\n- **Better Adaptability**: RAG can be fine-tuned for specific industries like healthcare, finance, or legal domains, where accurate and reliable information is crucial.\\n\\n#### Applications of RAG\\n\\n- **Chatbots and Virtual Assistants**: Enhancing conversational AI with real-time knowledge.\\n- **Customer Support**: Providing precise and informed answers to customer inquiries.\\n- **Content Generation**: Assisting writers with fact-checked content.\\n- **Education and Research**: Offering well-researched summaries and explanations.\\n\\n### Challenges and Considerations',\n",
       "  'metadata': {'filename': 'rag.md',\n",
       "   'creation_time': '2025-02-07 15:19:43 IST',\n",
       "   'size_kb': '2.75 KB'}},\n",
       " {'content': '### Future Trends\\n\\nWith the rise of AI and large-scale machine learning applications, vector databases are becoming essential for managing and querying complex data efficiently. Their integration with deep learning models and real-time analytics is expected to grow further.',\n",
       "  'metadata': {'filename': 'vector_db.md',\n",
       "   'creation_time': '2025-02-07 15:48:22 IST',\n",
       "   'size_kb': '1.83 KB'}},\n",
       " {'content': '### Retrieval-Augmented Generation (RAG)\\n\\nRetrieval-Augmented Generation (RAG) is an advanced technique that combines the strengths of retrieval-based models and generative models to improve the quality and relevance of generated text. It enhances traditional language models by incorporating external knowledge, leading to more informed and contextually accurate responses.\\n\\n#### Components of RAG\\n\\n1. **Retriever**: This component searches a knowledge base (such as Wikipedia, domain-specific documents, or other large corpora) to fetch relevant information based on an input query.\\n2. **Generator**: The retrieved information is then passed to a generative model (such as GPT or BART), which uses it to generate a coherent and contextually relevant response.\\n\\n#### Working Mechanism',\n",
       "  'metadata': {'filename': 'rag.md',\n",
       "   'creation_time': '2025-02-07 15:19:43 IST',\n",
       "   'size_kb': '2.75 KB'}},\n",
       " {'content': '### Vector Databases\\n\\nVector databases are specialized data storage solutions designed to efficiently handle high-dimensional vector data. They are commonly used in applications like machine learning, recommendation systems, and similarity search.\\n\\n#### Key Features\\n\\n- **Efficient Similarity Search**: Supports fast nearest neighbor search using algorithms like HNSW (Hierarchical Navigable Small World) and FAISS (Facebook AI Similarity Search).\\n- **Scalability**: Optimized for handling large-scale datasets with millions or billions of vectors.\\n- **Indexing Mechanisms**: Utilizes various indexing techniques, including quantization and tree-based structures, for faster retrieval.\\n- **Integration with AI**: Commonly used in AI-powered applications such as image retrieval, NLP, and anomaly detection.\\n\\n#### Popular Vector Databases',\n",
       "  'metadata': {'filename': 'vector_db.md',\n",
       "   'creation_time': '2025-02-07 15:48:22 IST',\n",
       "   'size_kb': '1.83 KB'}},\n",
       " {'content': '#### Popular Vector Databases\\n\\n- **FAISS**: Developed by Facebook AI, designed for high-speed similarity search.\\n- **Annoy**: Developed by Spotify, optimized for memory-efficient searches.\\n- **Milvus**: Open-source, highly scalable for machine learning applications.\\n- **Weaviate**: Supports semantic search with knowledge graph integration.\\n- **Pinecone**: Managed cloud-based solution for large-scale vector search.\\n\\n#### Use Cases\\n\\n- **Recommendation Systems**: Matching users with relevant products or content.\\n- **Image and Video Search**: Finding similar images using feature vectors.\\n- **Natural Language Processing (NLP)**: Storing word embeddings for semantic search.\\n- **Anomaly Detection**: Identifying unusual patterns in high-dimensional data.\\n\\n### Future Trends',\n",
       "  'metadata': {'filename': 'vector_db.md',\n",
       "   'creation_time': '2025-02-07 15:48:22 IST',\n",
       "   'size_kb': '1.83 KB'}},\n",
       " {'content': '2. **Non-Comparison-Based Sorting**: These algorithms sort without direct element comparisons, often using auxiliary data structures.\\n   - **Counting Sort**: Uses counting arrays to determine element positions, efficient for small integer ranges with O(n + k) complexity.\\n   - **Radix Sort**: Sorts numbers digit by digit using stable sorting techniques like counting sort. Has O(nk) complexity.\\n   - **Bucket Sort**: Distributes elements into multiple buckets and sorts each bucket individually. Effective for uniform distributions.\\n\\n#### Choosing the Right Sorting Algorithm\\n\\n- **Small Datasets**: Insertion Sort or Selection Sort is preferable due to simplicity.\\n- **Large Datasets**: Merge Sort and Quick Sort offer efficient performance.\\n- **Nearly Sorted Data**: Insertion Sort performs well.\\n- **Memory Constraints**: Quick Sort (in-place sorting) is a better choice over Merge Sort.\\n\\n#### Applications of Sorting Algorithms',\n",
       "  'metadata': {'filename': 'sorting_algo.md',\n",
       "   'creation_time': '2025-02-07 15:20:24 IST',\n",
       "   'size_kb': '3.2 KB'}},\n",
       " {'content': '1. **Comparison-Based Sorting**: These algorithms compare elements to determine their order.\\n   - **Bubble Sort**: Repeatedly swaps adjacent elements if they are in the wrong order. Simple but inefficient with O(n²) complexity.\\n   - **Selection Sort**: Selects the smallest element from the unsorted portion and swaps it with the first unsorted element. Has O(n²) complexity.\\n   - **Insertion Sort**: Builds the sorted array one item at a time by inserting elements into their correct position. Efficient for small datasets, with O(n²) complexity.\\n   - **Merge Sort**: Uses a divide-and-conquer approach to split, sort, and merge arrays. Has O(n log n) complexity.\\n   - **Quick Sort**: Selects a pivot and partitions the array into elements smaller and larger than the pivot, recursively sorting them. Has an average complexity of O(n log n).\\n   - **Heap Sort**: Uses a binary heap data structure to extract the smallest or largest element iteratively. Runs in O(n log n) time.',\n",
       "  'metadata': {'filename': 'sorting_algo.md',\n",
       "   'creation_time': '2025-02-07 15:20:24 IST',\n",
       "   'size_kb': '3.2 KB'}},\n",
       " {'content': '3. **Polynomial Regression**: Models non-linear relationships by adding polynomial terms to the linear equation.\\n   - Equation: Y = β0 + β1X + β2X² + ... + βnXⁿ + ε\\n   - Used when data shows curvature rather than a straight-line relationship.\\n\\n4. **Ridge Regression**: A linear regression model with L2 regularization to prevent overfitting.\\n   - Adds a penalty term (λΣβ²) to the loss function.\\n   - Helps when multicollinearity is present in the data.\\n\\n5. **Lasso Regression**: Similar to Ridge Regression but uses L1 regularization.\\n   - Encourages sparsity by shrinking some coefficients to zero.\\n   - Useful for feature selection and reducing model complexity.\\n\\n6. **Elastic Net Regression**: Combines Ridge and Lasso regression techniques.\\n   - Provides the advantages of both L1 and L2 regularization.\\n   - Useful when there are correlated predictors.',\n",
       "  'metadata': {'filename': 'regression.md',\n",
       "   'creation_time': '2025-02-07 15:21:36 IST',\n",
       "   'size_kb': '3.84 KB'}}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['relevant_docs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Retrieval-Augmented Generation (RAG) is an advanced technique that combines retrieval-based models and generative models to improve the quality and relevance of generated text by incorporating external knowledge. Vector databases are specialized data storage solutions designed to efficiently handle high-dimensional vector data and support fast nearest neighbor search, making them essential for the retriever component in RAG to fetch relevant information from a knowledge base based on an input query.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
